<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Estimating Brain Activity with High Spatial and Temporal Resolution using a Naturalistic MEG-fMRI Encoding Model">
  <meta name="keywords" content="MEG-fMRI integration, encoding model, naturalistic experiments">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MEG-fMRI</title>

  <style> 
    .div-a{ float:left;width:49%;border:1px solid #000} 
    .div-b{ float:left;width:49%;border:1px solid rgb(170, 13, 13)} 
  </style> 
  <style>
    .img_columns {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 50vh; /* Adjust this to fit your needs */
    }
    .img-div-a, .img-div-b {
      flex: 0 0 49%;
      border: 1px solid;
      display: flex;
      justify-content: center;
      align-items: center;
    }
    .img-div-a {
      border-color: #fff;
      height: 30vh;
    }
    .img-div-b {
      border-color: #fff;
      height: 50vh;
    }
    .highlight {
      color: red;
    }
  </style>
  </head> 

  <style>
    .video-container {
      display: flex;
      justify-content: space-between;
    }
    .video-container video {
      width: 48%; /* Adjust width as needed */
      height: auto;
    }
  </style>

  <style>
    .video-container-three {
      display: flex;
      justify-content: space-between;
    }
    .video-container-three video {
      width: 28%; /* Adjust width as needed */
      height: auto;
    }
  </style>

  <style>
    .small_video-container {
      display: flex;
      justify-content: space-between;
    }
    .small_video-container video {
      width: 23%; /* Adjust width as needed */
      height: auto;
    }
  </style>

  <style>
    .toggle-content {
        display: none;
        margin-top: 10px;
    }

    input[type="checkbox"] {
        display: none;
    }

    .toggle-label {
        cursor: pointer;
        color: rgb(111, 17, 17);
        text-decoration: bold;
        display: flex;
        align-items: center;
    }

    .toggle-label::before {
        content: 'â–¶';
        display: inline-block;
        margin-right: 5px;
        transition: transform 0.3s ease;
    }

    input[type="checkbox"]:checked + .toggle-label::before {
        transform: rotate(90deg);
    }

    input[type="checkbox"]:checked + .toggle-label + .toggle-content {
        display: block;
    }
  </style>

  <style>
    ul.custom-list {
      list-style-type: square;
    }
  </style>

  <style>
    .slide-content {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      text-align: center;
    }

    .slide-content video {
      width: 80%;
      max-width: 500px;
      height: auto;
      display: block;
      margin-top: 10px;
    }

    .subtitle {
      margin-bottom: 8px;
      font-size: 18px;
      font-weight: 500;
    }
  </style>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Estimating Brain Activity with High Spatial and Temporal Resolution using a Naturalistic MEG-fMRI Encoding Model
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Beige Jerry Jin, Leila Wehbe
            </span>
          </div>
          <div class="is-size-5 publication-affiliations">
            <span class="author-block">
              <p>Carnegie Mellon University</p>
            </span>
          </div>
          <div><a href="https://arxiv.org/abs/2510.09415">Paper (arXiv)</a></div>
          <div><a href="https://github.com/BeiGeJin/BeiGeJin.github.io/blob/master/assets/pdf/beige_MEGfMRI_poster.pdf">Poster (SNL2025)</a></div>
          <!-- <div><a href="https://github.com/BeiGeJin/MEG-fMRI">Github Repo</a></div> -->
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/imgs/teaser.png"
                type="image/jpg" width="80%" style="display: block; margin: 0 auto;">
      </img>
      <h2 class="context has-text-left">
        Our work integrates the millisecond-level temporal precision of MEG with the millimeter-scale spatial specificity of fMRI to reconstruct cortical source activity at a high spatiotemporal resolution in naturalistic experiments.      </h2>
    </div>
  </div>
</section>
<!--/ Teaser. -->


<!-- Abstract. -->
<section class="section abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current non-invasive neuroimaging techniques trade off between spatial resolution and temporal resolution. While magnetoencephalography (MEG) can capture rapid neural dynamics and functional magnetic resonance imaging (fMRI) can spatially localize brain activity, a unified picture that preserves both high resolutions remains an unsolved challenge with existing source localization or MEG-fMRI fusion methods, especially for single-trial naturalistic data. 
          </p>
          <p>
            We collected whole-head MEG when subjects listened passively to more than seven hours of narrative stories, using the same stimuli in an open fMRI dataset (LeBel et al., 2023). We developed a transformer-based encoding model that combines the MEG and fMRI from these two naturalistic speech comprehension experiments to estimate latent cortical source responses with high spatiotemporal resolution. Our model is trained to predict MEG and fMRI from multiple subjects simultaneously, with a latent layer that represents our estimates of reconstructed cortical sources.
          </p>
          <p>
            Our model predicts MEG better than the common standard of single-modality encoding models, and it also yields source estimates with higher spatial and temporal fidelity than classic minimum-norm solutions in simulation experiments. We validated the estimated latent sources by showing its strong generalizability across unseen subjects and modalities. Estimated activity in our source space predict electrocorticography (ECoG) better than an ECoG-trained encoding model in an entirely new dataset. By integrating the power of large naturalistic experiments, MEG, fMRI, and encoding models, we propose a practical route towards millisecond-and-millimeter brain mapping.
          </p>
        </div>
      </div>
    </div>
</section>
<!--/ Abstract. -->


<!--/ Model Architecture. -->
<section class="hero model">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3", style="margin-top: 60px;">Model Architecture</h2>
    </div>
    <div class="hero-body">
      <h2 class="content has-text-left">
        Feature streams enter the network through the input layer and traverse four transformer layers before being projected into the ``fsaverage'' source space by the source layer. The source estimates in the ``fsaverage'' source space is then transformed into subject-specific source estimates by the source morphing matrix. The MEG head predicts sensor signals by multiplying the source estimates with the lead-field matrix. The fMRI head predicts BOLD responses by convolving the downsampled envelope of the source estimates with a learnable hemodynamic response function (HRF) kernel. The MEG and fMRI of multiple subjects (e.g., S1, S2, ...) are predicted simultaneously. Under the joint constraints of MEG and fMRI from multiple subjects, our model recovers the source estimates with high spatiotemporal resolution.
      </h2>
      <img id="model" src="./static/imgs/model_structure.png"
                type="image/jpg" width="90%" style="display: block; margin: 0 auto;">
      </img>
    </div>
  </div>
</section>
<!--/ Model Architecture. -->


<!--/ Predictive Performance. -->
<section class="hero predict">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3", style="margin-top: 60px;">Predictive Performance</h2>
    </div>
    <div class="hero-body">
      <h2 class="content has-text-left">
        Our model is comparable to single-subject, single-modality ridge models which serve as a ceiling in MEG and fMRI prediction.
      </h2>
      <img id="predict" src="./static/imgs/model_prediction.png"
                type="image/jpg" width="90%" style="display: block; margin: 0 auto;">
      </img>
    </div>
  </div>
</section>
<!--/ Predictive Performance. -->


<!--/ ECoG Prediction. -->
<section class="hero ecog">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3", style="margin-top: 60px;">ECoG Prediction</h2>
    </div>
    <div class="hero-body">
      <h2 class="content has-text-left">
        Our model can produce powerful predictions of ECoG signal across experiments and subjects, outperforming models trained directly on ECoG data.
      </h2>
      <img id="ecog" src="./static/imgs/ecog.png"
                type="image/jpg" width="90%" style="display: block; margin: 0 auto;">
      </img>
    </div>
  </div>
</section>
<!--/ ECoG Prediction. -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
